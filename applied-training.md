‚Äî
–≠–Ω—ç—Ö“Ø“Ø –æ—Ä—á—É—É–ª–≥–∞ –Ω—å MIT –ª–∏—Ü–µ–Ω–∑–∏–π–Ω –¥–∞–≥—É—É —ç—Ö –±“Ø—Ç—ç—ç–ª—ç—ç—Å —Ö”©—Ä–≤“Ø“Ø–ª—ç–≤.
–≠—Ö —Å—É—Ä–≤–∞–ª–∂: Austin et al., "How to Scale Your Model" (https://jax-ml.github.io/scaling-book/)
–û—Ä—á.: Mongolian (mn)
‚Äî

---
layout: distill
title: "TPU –¥—ç—ç—Ä LLaMA 3-–≥ —Å—É—Ä–≥–∞—Ö"
# permalink: /main/
description: "”®–º–Ω”©—Ö —Ö—ç—Å—ç–≥—Ç —Å—É—Ä—Å–∞–Ω –∑“Ø–π–ª—ç—ç –∞—à–∏–≥–ª–∞–Ω TPU v5p –¥—ç—ç—Ä LLaMA 3 –∑–∞–≥–≤–∞—Ä—É—É–¥—ã–≥ —Ö—ç—Ä—Ö—ç–Ω —Å—É—Ä–≥–∞—Ö—ã–≥ –Ω–∞—Ä–∏–π–≤—á–ª–∞–Ω —Ö–∞—Ä—Ü–≥–∞–∞—è. –≠–¥–≥—ç—ç—Ä –∑–∞–≥–≤–∞—Ä—É—É–¥ —Ö—ç—Ä —Ç–æ–º –≤—ç? –Ø–Ω–∑ –±“Ø—Ä–∏–π–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ–Ω–¥ —Å—É—Ä–≥–∞—Ö –Ω—å —Ö—ç—Ä “Ø–Ω—ç—Ç—ç–π –≤—ç? –≠–¥–≥—ç—ç—Ä–∏–π–≥ —Ö—ç—Ä—Ö—ç–Ω —Ö—É–≤–∞–∞–¥–∞–≥ –≤—ç? ”®–º–Ω”©—Ö —Ö—ç—Å–≥“Ø“Ø–¥–∏–π–≥ –±–æ–¥–∏—Ç –∑–∞–≥–≤–∞—Ä –¥—ç—ç—Ä —Ö—ç—Ä—Ö—ç–Ω —Ö—ç—Ä—ç–≥–∂“Ø“Ø–ª—ç—Ö–∏–π–≥ —ç–Ω–≥–∏–π–Ω —Ç–æ–æ—Ü–æ–æ–ª–ª–æ–æ—Ä —Ö–∞—Ä—Ü–≥–∞–∞—è."
date: 2025-02-04
future: true
htmlwidgets: true
hidden: false

section_number: 6

previous_section_url: "../training"
previous_section_name: "5-—Ä —Ö—ç—Å—ç–≥: –°—É—Ä–≥–∞–ª—Ç"

next_section_url: ../inference
next_section_name: "7-—Ä —Ö—ç—Å—ç–≥: –î“Ø–≥–Ω—ç–ª—Ç"

–Ω–æ–º –∑“Ø–π: main.bib

giscus_comments: true

authors:
  - name: –ñ–µ–π–∫–æ–± –û—Å—Ç–∏–Ω
    url: "https://www.jacobaustin.org/"
    affiliations:
      name: Google DeepMind
  - name: –®–æ–ª—Ç–æ –î—É–≥–ª–∞—Å
    url: "https://x.com/_sholtodouglas"
  - name: –†–æ–π –§—Ä–æ—Å—Ç–∏–≥
    url: "https://cs.stanford.edu/~rfrostig/"
  - name: –ê–Ω—Å–µ–ª—å–º –õ–µ–≤—Å–∫–∞—è
    url: "https://anselmlevskaya.com/"
  - name: –ß–∞—Ä–ª–∏ –ß–µ–Ω
    url: "https://x.com/charliexychen"
  - name: –®–∞—Ä–∞–¥ –í–∏–∫—Ä–∞–º
    url: "https://sharadvikram.com/"
  - name: –§–µ–¥–µ—Ä–∏–∫–æ –õ–µ–±—Ä–æ–Ω
    url: "https://fedelebron.com/"
  - name: –ü–∏—Ç–µ—Ä –ß–æ–π
    url: "https://x.com/pchoy95"
  - name: –í–∏–Ω–∞–π –†–∞–º–∞—Å–µ—à
    url: "https://x.com/vinayramasesh"
  - name: –ê–ª—å–±–µ—Ä—Ç –í–µ–±—Å–æ–Ω
    url: "https://representation.ai/"
  - name: –†–∞–π–Ω–µ—Ä –ü–æ–ø–µ<sup>*</sup>
    url: https://x.com/reinerpope

# ”®”©—Ä–∏–π–Ω –±–∏—á–ª—ç–≥—Ç –∞–≥—É—É–ª–≥—ã–Ω –∂–∞–≥—Å–∞–∞–ª—Ç –Ω—ç–º—ç—Ö.
#   - –ê–≥—É—É–ª–≥—ã–Ω –∂–∞–≥—Å–∞–∞–ª—Ç—ã–Ω –Ω—ç—Ä“Ø“Ø–¥ –Ω—å —Ç—É—Ö–∞–π–Ω —Ö—ç—Å–≥–∏–π–Ω –Ω—ç—Ä—Ç—ç–π —è–≥ –∏–∂–∏–ª –±–∞–π—Ö —ë—Å—Ç–æ–π
#     –∏–Ω–≥—ç—Å–Ω—ç—ç—Ä –±–∏—á–ª—ç–≥ –¥–æ—Ç–æ—Ä—Ö —Ö–æ–ª–±–æ–æ—Å—É—É–¥ –∑”©–≤ –∞–∂–∏–ª–ª–∞—Ö –±–æ–ª–Ω–æ.
#   - –ì–∞—Ä –∞—Ä–≥–∞–∞—Ä markdown –∞–≥—É—É–ª–≥—ã–Ω –∂–∞–≥—Å–∞–∞–ª—Ç —Ö–∏–π—Ö–∏–π–Ω –æ—Ä–æ–Ω–¥ —ç–Ω—ç —Ñ–æ—Ä–º–∞—Ç—ã–≥ –∞—à–∏–≥–ª–∞–Ω–∞ —É—É.
toc:
  - name: "LLaMA 3 —è–º–∞—Ä —Ö–∞—Ä–∞–≥–¥–¥–∞–≥ –≤—ç?"
  - name: "–ü–∞—Ä–∞–º–µ—Ç—Ä –±–∞ FLOPs —Ç–æ–æ–ª–æ—Ö"
  - name: "LLaMA 3-70B-–≥ —Å—É—Ä–≥–∞–ª—Ç–∞–Ω–¥ —Ö—ç—Ä—Ö—ç–Ω —Ö—É–≤–∞–∞—Ö –≤—ç"
  - name: "–ë–æ–¥–ª–æ–≥—ã–Ω –∂–∏—à—ç—ç"

# –î–æ–æ—Ä –Ω—ç–º—ç–ª—Ç –ø–æ—Å—Ç–æ–¥ –∑–æ—Ä–∏—É–ª—Å–∞–Ω —Ç—É—Å–≥–∞–π –∑–∞–≥–≤–∞—Ä (styles) —Ö—ç—Ä—Ö—ç–Ω –æ—Ä—É—É–ª–∞—Ö—ã–≥ —Ö–∞—Ä—É—É–ª—Å–∞–Ω –∂–∏—à—ç—ç –±–∞–π–Ω–∞.
# –≠–Ω—ç –Ω—å —ç–Ω—ç –ø–æ—Å—Ç—ã–Ω 'Layouts' —Ö—ç—Å—ç–≥—Ç –∞—à–∏–≥–ª–∞–≥–¥–¥–∞–≥.
# –•—ç—Ä–≤—ç—ç —Ç–∞ —ç–Ω—ç –ø–æ—Å—Ç—ã–≥ –∑–∞–≥–≤–∞—Ä (template) –±–æ–ª–≥–æ–Ω –∞—à–∏–≥–ª–∞—Ö –±–æ–ª —ç–Ω—ç _styles –±–ª–æ–∫–∏–π–≥ —É—Å—Ç–≥–∞–∞—Ä–∞–π.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

_–≠–Ω—ç —Ö—ç—Å–≥–∏–π–Ω –∑–æ—Ä–∏–ª–≥–æ –±–æ–ª ”©–º–Ω”©—Ö —Ö—ç—Å–≥—ç—ç—Å –∞–≤—Å–∞–Ω “Ø—Ä –¥“Ø–Ω–≥ –º–∞—à –ø—Ä–∞–∫—Ç–∏–∫ –∞—Å—É—É–¥–∞–ª–¥ —Ö—ç—Ä—ç–≥–ª—ç—Ö —è–≤–¥–∞–ª —é–º: LLaMA 3 –∑–∞–≥–≤–∞—Ä—ã–Ω –±“Ø–ª–≥–∏–π–≥ (herd) —Å—É—Ä–≥–∞—Ö. ”®–º–Ω”©—Ö —Ö—ç—Å–≥“Ø“Ø–¥—ç—ç—Å —è–ª–≥–∞–∞—Ç–∞–π –Ω—å —ç–Ω—ç —É–¥–∞–∞ –±–∏–¥ –∏—Ö—ç–Ω—Ö –∞–∂–ª—ã–≥ ”©”©—Ä”©”© —Ö–∏–π—Ö–∏–π–≥ —Ö“Ø—Å—ç–∂ –±–∞–π–Ω–∞. –≠–Ω—ç —à–∞–ª—Ç–≥–∞–∞–Ω—ã —É–ª–º–∞–∞—Å –±–∏–¥ —Ö–∞—Ä–∏—É–ª—Ç—É—É–¥—ã–≥ –Ω—É—É—Å–∞–Ω –±–∞–π–≥–∞–∞, –∏–Ω–≥—ç—Å–Ω—ç—ç—Ä —Ç–∞ —ç—Ö–ª—ç—ç–¥ ”©”©—Ä”©”© —Ö–∞—Ä–∏—É–ª–∞—Ö—ã–≥ –æ—Ä–æ–ª–¥–æ–æ—Ä–æ–π. “Æ–∑—ç–≥ –∞–≤–∞–∞–¥ –≥–∞—Ä–∞–∞—Ä –±–æ–¥–æ–æ–¥ “Ø–∑—ç—ç—Ä—ç–π!_

### LLaMA 3 —è–º–∞—Ä —Ö–∞—Ä–∞–≥–¥–¥–∞–≥ –≤—ç?

LLaMA-3 –∑–∞–≥–≤–∞—Ä—ã–Ω –≥—ç—Ä –±“Ø–ª<d-cite key="llama3"></d-cite> –Ω—å 3 “Ø–Ω–¥—Å—ç–Ω –∑–∞–≥–≤–∞—Ä—Ç–∞–π: LLaMA 3 8B, 70B, –±–æ–ª–æ–Ω 405B. –ë–∏–¥ –≥–æ–ª—á–ª–æ–Ω 70B –∑–∞–≥–≤–∞—Ä—Ç –∞–Ω—Ö–∞–∞—Ä–Ω–∞, —Ö–∞—Ä–∏–Ω 8B –±–æ–ª–æ–Ω 405B –∑–∞–≥–≤–∞—Ä—ã–≥ —Ç–∞ –Ω–∞—Ä —Å“Ø“Ø–ª–∏–π–Ω —Ö—ç—Å–≥–∏–π–Ω –±–æ–¥–ª–æ–≥—ã–Ω —Ö—ç—Å—ç–≥—Ç ”©”©—Ä—Å–¥”©”© —Å—É–¥–∞–ª–Ω–∞. –≠–Ω–¥ LLaMA 3-70B –∑–∞–≥–≤–∞—Ä—ã–Ω –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –±–∞–π–Ω–∞. –≠–Ω—ç –Ω—å LLaMA-–≥–∏–π–Ω [HuggingFace —Ö—É—É–¥–∞—Å–Ω–∞–∞—Å](https://huggingface.co/meta-llama/Meta-Llama-3-70B/blob/main/config.json) –∞–≤—Å–∞–Ω –±–æ–ª–Ω–æ.

| **hyperparam**              | **—É—Ç–≥–∞** |
| --------------------------- | --------- |
| $$n_\text{layers}$$ (L)     | 80        |
| $$d_\text{model}$$ (D)      | 8,192     |
| $$d_{ff}$$ (F)              | 28,672    |
| $$n_\text{heads}$$ (N)      | 64        |
| $$n_\text{kv_heads}$$ (K)   | 8         |
| $$d_\text{qkv}$$ (H)        | 128       |
| $$n_\text{embeddings}$$ (V) | 128,256   |

To highlight how easy this is to find, here's the config itself, along with a mapping:

{% include figure.liquid path="assets/img/llama-json.png" class="img-fluid" %}

_It's useful to make a big table with these numbers for many different open-source LLMs, so you can quickly compare the design decisions they've made._

### Counting parameters and FLOPs

**Question:** From this table, can we calculate the LLaMA 3-70B parameter count? ü§´ Let's apply the content of [Section 4](../transformers) and see if we can get 70B!

| param            | formula                                                                                                                                           | count                                                        |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |
| FFW params       | d_model * d_ff * 3 (for gelu + out-projection) * n_layers                                                                                         | 8,192 * 8,192 * 3.5 * 3 * 80 = **56.3e9**                    |
| Vocab params     | 2 (input and output embeddings) * n_embeddings * d_model                                                                                          | 2 * 128,256 * 8,192 = **2.1e9**                              |
| Attention params | n_layers * [ 2 (for q embedding and concatenated output projection) * d_model * n_heads * d_qkv + 2 (for k and v) * d_model * n_kv_heads * d_qkv] | 80 * (2 * 8,192 * 64 * 128 + 2 * 8,192 * 8 * 128) = **12e9** |
|                  |                                                                                                                                                   | 56.3e9 + 2.1e9 + 12e9 = **70.4e9**                           |

That's great! We get the number we expect. You'll notice as expected that the FFW parameters totally dominate the overall parameter count, although attention is non-trivial.

<p markdown=1 class="takeaway">**Takeaway**: The 3 big weight matrices in the MLP block are so much larger than all the other arrays in the Transformer that we can typically almost ignore all other parameters when reasoning about model memory or FLOPs. For LLaMA 3-70B, they represent 56B of 70B parameters.</p>

Let's look at FLOPs now! *Remember the general rules for training from [Section 4](../transformers).*

**Question:** How many FLOPs does LLaMA-3 perform per token per training step? _This helps us determine how expensive the whole training process will be._

{% details Click here for the answer, once you've thought about it! %}

**Answer**: As shown in [Section 4](../transformers), we do roughly $$6 \cdot \text{param count}$$ FLOPs per token, so here that's roughly `6 * 70e9 = 4.2e11` FLOPs / token. That's about half a TFLOP per token per step. Assuming we're compute-bound, this should take roughly `4.2e11 / 4.59E+14 = 1ms` on a single TPU v5p chip, assuming perfect FLOPs utilization.

{% enddetails %}

**Question:** LLaMA 3 was trained for about 15 trillion tokens. How many FLOPs is that total?

{% details Click here for the answer, once you've thought about it! %}

**Answer**: That's easy, it's just `4.2e11 * 15e12 = 6.3e24 FLOPs` total. 6.3 yottaFLOPs. That's a lot! On a single TPU this would take `6.3e24 / 4.59E+14 = 435 years`. That's also a lot!

{% enddetails %}

**Question:** Let's say we wanted to train on a full TPU v5p pod with 16x20x28 = 8960 chips. How long would this take to train at 40% MFU in bfloat16, assuming we are compute-bound?

{% details Click here for the answer, once you've thought about it! %}

**Answer**: We know that each TPU v5p can perform 4.59e14 FLOPs / second. At 40% MFU, this will take about `T = 6.3e24 / (8960 * 4.59e14 * 0.4) = 3.8e6 seconds`. **This is about 44 days!** That's fairly reasonable, assuming we can actually achieve 40% MFU.

{% enddetails %}

**Question:** LLaMA 3-70B was pretrained with a batch size of about 4M tokens. How many TPUs do we need at minimum to train with this batch size? _You can assume bfloat16 parameters and float32 optimizer state, and that you checkpoint gradients 4 times per layer._

{% details Click here for the answer, once you've thought about it! %}

**Answer**: This question is primarily asking about memory usage, since that's the only strict constraint on available compute. During training, we have three primary uses of HBM: model parameters, optimizer state, and gradient checkpoints. If we assume bfloat16 weights, float32 optimizer state, and a _very_ conservative gradient checkpointing scheme (4 times per layer), we have:

| **Params** | 2 * 70GB | ~140GB |
| **Optimizer State** | 8 * 70GB | ~560GB |
| **Gradient Checkpoints** | 2 * 8192 * 4e6 * 4 * 80 | ~20.9TB |
| **Total**                |                         | ~21.6TB |

The total here is about 21.6TB. You notice that gradient checkpointing strongly dominates the memory picture, even with a very conservative checkpointing scheme. We could technically go to 1 checkpoint per layer, or do microbatching, but this is a reasonable picture. With these assumptions, since each TPU v5p has 96GB of HBM, we need `21.6e12 / 96e9 = 225` TPUs. That's not very much actually!

*Why wouldn't we do this?* Well, because it would take us `44 days * 8960 / 225 = 1752 days` to train. That's nearly four years. **That's a lot.** Still, this makes it clear that we're using these large clusters not because we're bound by memory but rather because we need the extra FLOPs.

{% enddetails %}

**Question:** Under the same assumptions as the question above, if we use 8960 TPU v5p chips, how much memory will we use per-chip?

{% details Click here for the answer, once you've thought about it! %}

**Answer**: Our total memory is still about 21.6TB, so per-chip we'll be using about 2.4GB per chip, which is bascially nothing. If we did much more aggressive checkpointing, e.g. 12 checkpoints per layer, we'd still only be at 8GB per chip. We're nowhere near being memory bound during training at these scales.

{% enddetails %}

<p markdown=1 class="takeaway">**Takeaways**: It is technically possible to train even very large models on very small topologies, with the caveat that they will likely take a long time. Being able to calculate the total FLOPs of a training run allows us to ballpark its training time by assuming a modest MFU and a known topology.</p>

### How to shard LLaMA 3-70B for training

Let's stick to our setting from above and say we want to train LLaMA 3-70B with 4M token batch size (1024 sequences of length 4096 per batch) on a TPU v5p pod of 8960 chips. Let's discuss what the best sharding strategy is for this model.

**Question:** Under the assumptions above, can we train our model with FSDP alone? To start, let's say we can't do any sequence/context parallelism. _This should be the first idea you have, since it's simple and will introduce no extra communication if it works._

{% details Click here for the answer, once you've thought about it! %}

**Answer**: This answer will be a little pedantic. As noted above, LLaMA 3-70B is initially trained with sequences of length 4K, so the batch size of 4M tokens gives us a *sequence batch size* of 1024. That means we can only really do pure data parallelism/FSDP up to 1024 chips _because that's how many sequences we have to do data parallelism over_. So the answer in the simple sense of "fully data parallelism with no extra communication" is no. The next question will answer a slightly less pedantic version of this.

{% enddetails %}

**Question:** Let's relax the requirement of not doing any sequence sharding. If we allow ourselves to do FSDP over both the batch _and_ sequence axes, can we train LLaMA 3-70B with only FSDP on 8960 chips?

{% details Click here for the answer, once you've thought about it! %}

**Answer**: Now that we're allowing ourselves to do sequence/context parallelism as well, we can scale up way more. First let's calculate our per-device batch size. If we do 8960-way FSDP, we end with a per-TPU batch size of `4 * 1024 * 1024 / 8960 = 468 tokens`. We know from the previous section that we become ICI-bound by FSDP when $$\text{per device batch size} < 2550 / M_X$$. Since we can dedicate 3 axes here with a full 3D pod, this would give us a lower bound of 850, which we're well below. **So the answer is no, even with 3 axes. We would be solidly communication-bound.**

{% enddetails %}

**Question:** Now let's look at mixed tensor parallelism and FSDP. Does there exist some combination that lets us remain compute-bound? What amount of FSDP and tensor parallelism should we do if so?

{% details Click here for the answer, once you've thought about it! %}

**Answer**: First let's check to see if this will even fit. We know that we'll be comms-bound if our per-chip batch size is less than $2550^2 / 2F = 113$. As we saw above, we're slightly above this. So that's great! Now to pick the optimal amount of FSDP, we can use the formula

$$X_{opt} = \sqrt{\frac{2BN}{F}} = \sqrt{\frac{2 \cdot 4.19e6 \cdot 8960}{28672}} = 1618$$

2-—ã–Ω –æ–π—Ä–æ–ª—Ü–æ–æ —É—Ç–≥–∞–¥ –æ–π—Ä—Ç—É—É–ª–∂ —Ç–æ–π–º–ª–æ–Ω –±–æ–¥–≤–æ–ª, –±–∏–¥—ç–Ω–¥ –æ–π—Ä–æ–ª—Ü–æ–æ–≥–æ–æ—Ä 2048 FSDP –±–æ–ª–æ–Ω 4 tensor parallelism –±–∞–π–Ω–∞. –≠–Ω—ç –Ω—å —Å–∞–π–Ω –∞–∂–∏–ª–ª–∞—Ö —ë—Å—Ç–æ–π!

{% enddetails %}

<p markdown=1 class="takeaway">**–ì–æ–ª —Å–∞–Ω–∞–∞**: –ë–∏–¥ LLaMA-3 –∑–∞–≥–≤–∞—Ä—ã–≥ 4 —Å–∞—è —Ç–æ–∫–µ–Ω –±“Ø—Ö–∏–π batch size-—Ç–∞–π–≥–∞–∞—Ä –±“Ø—Ç—ç–Ω TPU v5p pod –¥—ç—ç—Ä —Å—É—Ä–≥–∞–ª—Ç —Ö–∏–π–∂ —á–∞–¥–Ω–∞. “Æ“Ø–Ω–¥ ”©–≥”©–≥–¥–ª–∏–π–Ω –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º (1024-—É–¥–∞–∞), –¥–∞—Ä–∞–∞–ª–ª—ã–Ω –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º (2-—É–¥–∞–∞), –±–æ–ª–æ–Ω —Ç—ç–Ω—Ü—ç—Ä–∏–π–Ω –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º (4-—É–¥–∞–∞) —Ö–æ—Å–ª—É—É–ª–∂ —Ö—ç—Ä—ç–≥–ª—ç–Ω—ç. –ò–Ω–≥—ç—Å–Ω—ç—ç—Ä —Ö–∞—Ä–∏–ª—Ü–∞–∞–Ω—ã (communication) –∞—Å—É—É–¥–∞–ª “Ø“Ø—Å—ç—Ö–≥“Ø–π. –•—ç—Ä–≤—ç—ç –∑”©–≤—Ö”©–Ω FSDP —ç—Å–≤—ç–ª FSDP + –¥–∞—Ä–∞–∞–ª–ª—ã–Ω –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —Ö—ç—Ä—ç–≥–ª—ç–≤—ç–ª –±–∏–¥ —Ö–∞—Ä–∏–ª—Ü–∞–∞–Ω—ã –∞—Å—É—É–¥–∞–ª—Ç–∞–π –±–æ–ª–Ω–æ. ”®–º–Ω”©—Ö —Ö—ç—Å—ç–≥—Ç –≥–∞—Ä–≥–∞—Å–∞–Ω —Ç–æ–º—ä—ë–æ–Ω—É—É–¥ –º–∞—à —Ö—ç—Ä—ç–≥—Ç—ç–π, –ø—Ä–∞–∫—Ç–∏–∫ —é–º.</p>

## –ê–∂–∏–ª–ª–∞—Å–∞–Ω –±–æ–¥–ª–æ–≥—É—É–¥

**–ê—Å—É—É–ª—Ç 1 [LLaMA 70B-–≥ –æ–ª–æ–Ω —á–∏–ø –¥—ç—ç—Ä ”©—Ä–≥”©–∂“Ø“Ø–ª—ç—Ö]:** –ë–∏–¥ LLaMA 3-70B-–≥ 4 –ø–æ–¥ –¥—ç—ç—Ä –∏–∂–∏–ª batch size-—Ç–∞–π–≥–∞–∞—Ä —Å—É—Ä–≥–∞—Ö—ã–≥ —Ö“Ø—Å–≤—ç–ª —è–º–∞—Ä –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º—ã–Ω —Å—Ö–µ–º –∞—à–∏–≥–ª–∞—Ö –≤—ç? –ë–∏–¥ —Ç–æ–æ—Ü–æ–æ–ª–æ–ª (compute) —ç—Å–≤—ç–ª —Ö–∞—Ä–∏–ª—Ü–∞–∞ —Ö–æ–ª–±–æ–æ (communication)-–Ω–¥ —Ö—è–∑–≥–∞–∞—Ä–ª–∞–≥–¥–∞—Ö —É—É? –°—É—Ä–≥–∞—Ö —Ö—É–≥–∞—Ü–∞–∞ –æ–π—Ä–æ–ª—Ü–æ–æ–≥–æ–æ—Ä —Ö—ç—Ä —É–¥–∞–∞–Ω –±–∞–π—Ö –≤—ç? *–ó”©–≤ roofline bound-—ã–≥ –∞—à–∏–≥–ª–∞—Ö–∞–∞ –º–∞—Ä—Ç—É—É–∑–∞–π.*

**–ê—Å—É—É–ª—Ç 2 [LLaMA 405B]:**

(a) LLaMA 3-405B [config](https://huggingface.co/meta-llama/Llama-3.1-405B/blob/main/config.json)-–∏–π–≥ –∞—à–∏–≥–ª–∞–Ω, –¥—ç—ç—Ä—Ö—Ç—ç–π –∞–¥–∏–ª –±“Ø—Ö –≥–æ–ª hyperparameter-—É—É–¥—ã–≥ –∞–≥—É—É–ª—Å–∞–Ω —Ö“Ø—Å–Ω—ç–≥—Ç –±–∏—á–Ω—ç “Ø“Ø. –≠–Ω—ç –∑–∞–≥–≤–∞—Ä –Ω–∏–π—Ç —Ö—ç–¥—ç–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä—Ç—ç–π –≤—ç? –ù—ç–≥ —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∞–ª—Ö–∞–º–¥ —Ö—ç–¥—ç–Ω FLOP –∑–∞—Ä—Ü—É—É–ª–¥–∞–≥ –≤—ç? –•—ç—Ä–≤—ç—ç –±–∏–¥ 15T —Ç–æ–∫–µ–Ω –¥—ç—ç—Ä —Å—É—Ä–≥–∞—Ö –±–æ–ª –Ω–∏–π—Ç —Ö—ç–¥—ç–Ω FLOP –∑–∞—Ä—Ü—É—É–ª–∞—Ö –≤—ç?

(–±) –ë–∏–¥ 8 TPU v5p pod –¥—ç—ç—Ä —Å—É—Ä–≥–∞–ª—Ç —Ö–∏–π—Ö–∏–π–≥ —Ö“Ø—Å—ç–∂ –±–∞–π–Ω–∞ –≥—ç–∂ “Ø–∑—å–µ. –Ø–º–∞—Ä –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º—ã–Ω —Å—Ö–µ–º –∞—à–∏–≥–ª–∞—Ö –≤—ç? –°—É—Ä–≥–∞–ª—Ç —Ö—ç—Ä —É–¥–∞–∞–Ω “Ø—Ä–≥—ç–ª–∂–ª—ç—Ö –≤—ç? –¢–æ–æ—Ü–æ–æ–ª–æ–ª (compute) —ç—Å–≤—ç–ª —Ö–∞—Ä–∏–ª—Ü–∞–∞ —Ö–æ–ª–±–æ–æ (comms) –∞–ª—å –Ω—å —Ö—è–∑–≥–∞–∞—Ä–ª–∞—Ö –≤—ç?

<h3 markdown=1 class="next-section">–≠–Ω—ç –±–æ–ª 6-—Ä —Ö—ç—Å–≥–∏–π–Ω –±“Ø—Ö –∑“Ø–π–ª. 7-—Ä —Ö—ç—Å—ç–≥ –±—É—é—É Transformer inference-–∏–π–Ω —Ç—É—Ö–∞–π –±–æ–ª [—ç–Ω–¥](../inference) –¥–∞—Ä–Ω–∞ —É—É.</h3>
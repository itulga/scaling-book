—
Энэхүү орчуулга нь MIT лицензийн дагуу эх бүтээлээс хөрвүүлэв.
Эх сурвалж: Austin et al., "How to Scale Your Model" (https://jax-ml.github.io/scaling-book/)
Орч.: Mongolian (mn)
—

---
layout: distill
title: "Загвараа хэрхэн томруулах вэ"
subtitle: "TPU дээр LLM-уудыг системийн үүднээс харах нь"
# permalink: /main/
description: "LLM-уудыг сургах нь ихэнхдээ ид шид шиг санагддаг ч, загварынхаа гүйцэтгэлийг ойлгож, оновчтой болгох нь тийм ч хэцүү биш. Энэ ном нь хэлний загваруудыг томруулах шинжлэх ухааныг ойлгомжтой болгоход зориулагдсан: TPU (мөн GPU)-ууд хэрхэн ажилладаг, тэд хэрхэн хоорондоо мэдээлэл солилцдог, LLM-ууд бодит төхөөрөмж дээр хэрхэн ажилладаг, мөн сургах болон таамаглах үед загваруудаа хэрхэн зэрэгцүүлэн ажиллуулах вэ гэдгийг тайлбарлана. Ингэснээр маш их хэмжээний өгөгдөл дээр үр дүнтэй ажиллуулах боломжтой болно. Хэрвээ та “энэ LLM-ыг сургахад хэр үнэтэй байх ёстой вэ”, эсвэл “энэ загварыг өөрөө ажиллуулахад надад хэр их санах ой хэрэгтэй вэ”, эсвэл “AllGather гэж юу вэ” гэж бодож байсан бол энэ ном танд хэрэгтэй гэж найдаж байна."
date: 2025-02-04
future: true
htmlwidgets: true
hidden: false

giscus_comments: true

хэсгийн_дугаар: 0

previous_section_url: ""
previous_section_name: "0-р хэсэг: Танилцуулга"

next_section_url: roofline
next_section_name: "1-р хэсэг: Rooflines"

ном зүй: main.bib

ишлэл: үнэн

authors:
  - name: Жэйкоб Остин
    url: "https://www.jacobaustin.org/"
    affiliations:
      name: Google DeepMind
  - name: Шолто Дуглас
    url: "https://x.com/_sholtodouglas"
  - name: Рой Фростиг
    url: "https://cs.stanford.edu/~rfrostig/"
  - name: Ансельм Левская
    url: "https://anselmlevskaya.com/"
  - name: Чарли Чен
    url: "https://x.com/charliexychen"
  - name: Шарад Викрам
    url: "https://sharadvikram.com/"
  - name: Федерико Леброн
    url: "https://fedelebron.com/"
  - name: Питер Чой
    url: "https://x.com/pchoy95"
  - name: Винай Рамасеш
    url: "https://x.com/vinayramasesh"
  - name: Альберт Вебсон
    url: "https://representation.ai/"
  - name: Рейнер Попе<sup>*</sup>
    url: https://x.com/reinerpope

# Өөрийн бичлэгт агуулгын жагсаалт нэмэх.
#   - Агуулгын жагсаалтын (TOC) нэрс нь хэсгүүдийн жинхэнэ нэртэй таарч байх ёстой.
#     Ингэснээр бичлэг доторх холбоосууд зөв ажиллана.
#   - Доорх форматыг ашиглана уу, гараар markdown агуулгын жагсаалт хийхээс зайлсхий.
toc:
  - name: Ерөнхий бүтэц
  - name: Хэсгүүд рүү холбоосууд

# Доор нэмэлт постод зориулсан тусгай загваруудыг хэрхэн оруулах жишээ байна.
# Энэ нь энэ постын 'Layouts' хэсэгт ашиглагддаг.
# Хэрвээ та энэ постыг загвар болгон ашиглах бол энэ _styles блокыг устгаарай.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

{% include figure.liquid path="assets/img/dragon.png" class="img-fluid" %}

Гүн сургалтын ихэнх нь одоо ч нэг төрлийн "хар ид шид" шиг санагддаг ч, таны загваруудын гүйцэтгэлийг сайжруулах нь тийм биш — том хэмжээтэй байсан ч гэсэн! Харьцангуй энгийн зарчмууд хаа сайгүй үйлчилдэг — нэг accelerator-тай ажиллахад ч, арван мянгаас дээш accelerator-тай ажиллахад ч — эдгээрийг ойлговол та олон хэрэгтэй зүйл хийж чадна:

- Таны загварын хэсгүүд онолын хамгийн сайн үр дүндөө хэр ойр байгааг ойролцоогоор тооцоол.
- Өөр өөр параллелизм схемүүдийг өөр өөр хэмжээст ухаалгаар сонго (тооцооллыг олон төхөөрөмж дээр хэрхэн хуваахаа шийдэх).
- Том Transformer загваруудыг сургах болон ажиллуулахад шаардагдах зардал, хугацааг тооцоол.
- [Тодорхой](https://arxiv.org/abs/2205.14135) [төхөөрөмжийн](https://arxiv.org/abs/1911.02150) [боломжуудыг](https://arxiv.org/abs/2007.00072) ашиглах алгоритмуудыг зохиох.
- Одоогийн алгоритмын гүйцэтгэлийг хязгаарлаж буй зүйлсийг ойлгож, түүнд тулгуурлан төхөөрөмж зохиох.

**Хүлээгдэж буй мэдлэгийн түвшин:** Бид таныг LLM-ууд болон Transformer архитектурын үндсэн ойлголттой гэж үзнэ. Гэхдээ эдгээр нь том хэмжээнд хэрхэн ажилладагийг заавал мэдэх шаардлагагүй. Та LLM сургалтын үндсийг мэддэг байх хэрэгтэй бөгөөд JAX-ийн талаар бага зэрэг мэдлэгтэй бол сайн. Ашигтай суурь мэдлэг авахын тулд [энэ блог бичлэг](https://jalammar.github.io/illustrated-transformer/)-ийг Transformer архитектурын талаар уншиж болно. Мөн [эхний Transformer өгүүлэл](https://arxiv.org/abs/1706.03762)-ийг уншаарай. Мөн илүү их хэрэгтэй, зэрэгцээ болон ирээдүйн унших материалуудыг [энэ жагсаалт](conclusion#further-reading)-аас үзээрэй.

**Зорилго ба санал хүсэлт:** Эцэст нь та өгөгдсөн hardware платформ дээр Transformer model-д хамгийн сайн parallelism схемийг хэрхэн тооцоолж болох талаар болон сургалт, inference хэр удаан үргэлжлэхийг ойролцоогоор мэддэг болсон байх ёстой. Хэрвээ ойлгоогүй бол бидэнд имэйл бичих эсвэл сэтгэгдэл үлдээгээрэй! Бид үүнийг хэрхэн илүү ойлгомжтой болгох талаар мэдэхийг хүсэж байна.

<p markdown=1 class="announce">Танд NVIDIA GPU-ийн тухай шинэ [12-р хэсэг](gpus)-ийг унших ч бас сонирхолтой байж магадгүй!</p>

### Яагаад анхаарах хэрэгтэй вэ?

Гурван эсвэл дөрвөн жилийн өмнө, ихэнх ML судлаачид энэ номын агуулгыг ойлгох шаардлагагүй байсан гэж би бодож байна. Гэвч өнөө үед “жижиг” загварууд хүртэл техник хангамжийн хязгаарт маш ойр ажиллаж байгаа тул шинэ судалгаа хийхийн тулд та үр ашигтай байдлыг том хэмжээнд бодох хэрэгтэй болжээ.<d-footnote>Түүхэн талаасаа, ML судалгаа нь системийн шинэчлэл ба програм хангамжийн сайжруулалтын хооронд “tick-tock” маягийн мөчлөг дагаж ирсэн. Алекс Крижевский CNN-ийг хурдан болгохын тулд маш төвөгтэй CUDA код бичих шаардлагатай байсан ч хэдхэн жилийн дараа Theano, TensorFlow зэрэг сангууд гарч ирсэн тул ийм зүйл хийх шаардлагагүй болсон. Магадгүй энэ тал дээр ч бас ийм зүйл тохиолдож, энэ номын бүх зүйл хэдэн жилийн дараа далдлагдах байх. Гэхдээ “scaling laws” нь бидний загваруудыг техник хангамжийн хамгийн захад байнга хүргэж байгаа бөгөөд ойрын ирээдүйд шинэ судалгаа хийхэд том техник хангамжийн топологи дээр загваруудыг үр ашигтай өргөжүүлэхийг ойлгох шаардлагатай байх нь тодорхой байна.</d-footnote> **Бенчмарк дээр 20% амжилт гаргасан ч, хэрвээ энэ нь roofline efficiency-г 20% бууруулж байвал ач холбогдолгүй.** Ирээдүйтэй загварын архитектурууд ихэвчлэн бүтэлгүйтдэг. Учир нь тэдгээр нь том хэмжээнд үр ашигтай ажиллаж чаддаггүй эсвэл хэн ч тэднийг үр ашигтай болгохын тулд ажил хийдэггүй.

**“Загварын өргөтгөл” (model scaling)-ын зорилго нь сургалт эсвэл таамаглалд ашиглагдах чипийн тоог нэмэгдүүлэх явдал бөгөөд ингэснээр дамжуулалтын хурд (throughput) нь шууд, шугаман өсөлттэй байх ёстой.** Үүнийг "*хүчтэй өргөтгөл*" (strong scaling) гэж нэрлэдэг. Илүү олон чип нэмэх ("параллелизм") нь ихэвчлэн тооцооллын хугацааг багасгадаг ч, энэ нь чипүүдийн хоорондын харилцааны нэмэлт зардлыг авчирдаг. Хэрвээ харилцаа холбоо нь тооцооллоос удаан болвол бид "харилцааны хязгаарлалтад" (communication bound) орж, хүчтэй өргөтгөлийг хийж чадахгүй.<d-footnote>Тооцооллын хугацаа багасах тусам, ихэвчлэн нэг чип дээр саатал үүсдэг. Таны шинэ TPU эсвэл GPU нь секундэд 500 их наяд үйлдэл хийх чадвартай гэж үнэлэгдсэн байж болох ч, хэрвээ та анхааралтай ажиллахгүй бол, санах ой дотор параметрүүдийг зөөхөд удааширвал энэ чадлынхаа аравны нэгийг л ашиглаж болно. Нэг чипийн тооцоолол, санах ойн зурвас (memory bandwidth), нийт санах ойн хэмжээний харилцан үйлчлэл нь өргөтгөлийн гол асуудал юм.</d-footnote> Хэрвээ бид техник хангамжаа сайн ойлгож, эдгээр саатал хаана үүсэхийг урьдчилан мэдэж чадвал, загвараа зохион бүтээх эсвэл дахин тохируулах замаар тэдгээрээс зайлсхийж чадна.<d-footnote>Техник хангамжийн зохион бүтээгчид эсрэг асуудалтай тулгардаг: манай алгоритмуудад хангалттай тооцоолол, зурвас, санах ой өгөх, гэхдээ зардлыг багасгах хэрэгтэй. Энэ "хамтран зохиох" (co-design) асуудал ямар их дарамттай болохыг та төсөөлж болно: анхны чипүүд гарах үед (ихэвчлэн 2-3 жилийн дараа) алгоритмууд ямар байхыг таах хэрэгтэй болдог. TPU-гийн түүх энэ тоглоомд амжилттай болсон. Матриц үржүүлэх (matrix multiplication) нь бараг бүх алгоритмаас илүү их FLOPs-ыг нэг байт санах ойд ашигладаг өвөрмөц алгоритм бөгөөд анхны TPU болон түүний systolic array архитектур нь тухайн үедээ GPU-гаас илүү гүйцэтгэл / $-ийг үзүүлсэн. TPU-г ML ажилд зориулж бүтээсэн, харин GPU-нууд TensorCore-оороо энэ салбарт хурдан өөрчлөгдөж байна. Гэхдээ хэрвээ нейрон сүлжээ амжилт олоогүй бол, эсвэл үндсэн зарчим нь өөрчлөгдсөн бол (TPU нь GPU-аас уян хатан биш), энэ нь маш их зардалтай байх байсан.</d-footnote>

*Бидний энэ номын зорилго бол TPU (мөн GPU) техник хангамж хэрхэн ажилладаг болон Transformer архитектур хэрхэн хөгжиж, одоогийн техник хангамж дээр сайн ажиллах болсон тухай тайлбарлах юм. Энэ нь шинэ архитектур зохиож буй судлаачид болон одоогийн үеийн LLM-уудыг хурдан ажиллуулахыг хүсэж буй инженерүүдэд хэрэгтэй гэж найдаж байна.*

## Өндөр түвшний төлөвлөгөө

Энэ номын ерөнхий бүтэц дараах байдалтай байна:

[1-р хэсэг](roofline) нь roofline шинжилгээ болон бидний өргөжүүлэх чадварыг хязгаарлаж болох хүчин зүйлсийг (харилцаа, тооцоолол, санах ой) тайлбарлана. [2-р хэсэг](tpus) болон [3-р хэсэг](sharding) нь TPU хэрхэн ажилладаг талаар дэлгэрэнгүй ярилцана, тус тусдаа чипүүд болон — маш чухал — хоорондоо холбогдсон систем байдлаар, чип хоорондын холбоос нь хязгаарлагдмал зурвасын өргөн ба хүлээлгийн хугацаатай байдаг. Бид дараах асуултуудад хариулах болно:

* Тодорхой хэмжээтэй матриц үржүүлэхэд хэр хугацаа шаардагдах вэ? Ямар үед тооцоолол, санах ой, эсвэл холбооны зурвасын өргөнд хязгаарлагдах вэ?
* TPU-уудыг сургалтын кластер үүсгэхэд хэрхэн холбодог вэ? Системийн хэсэг бүр ямар зурвасын өргөнтэй вэ?
* Олон TPU дээр массив цуглуулах, тараах, эсвэл дахин хуваарилахад хэр хугацаа ордог вэ?
* Өөр өөр төхөөрөмж дээр өөрөөр хуваарилагдсан матрицуудыг хэрхэн үр дүнтэй үржүүлэх вэ?

{% include figure.liquid path="assets/img/pointwise-product.gif" class="img-small" caption="<b>Зураг:</b> <a href='tpus'>2-р хэсэг</a>-ээс авсан диаграмм. Энэ зурагт TPU хэрхэн элементүүдийн үржвэрийг (elementwise product) хийж байгааг харуулсан байна. Манай массивуудын хэмжээ болон янз бүрийн холбоосуудын дамжуулах чадлаас (bandwidth) шалтгаалаад бид тооцооллын хязгаартай (compute-bound, бүх hardware-ийн тооцооллын чадлыг ашиглаж байгаа) эсвэл холбооны хязгаартай (comms-bound, санах ойн ачааллаас болж удааширч байгаа) байж болно." %}

Таван жилийн өмнө ML-д олон янзын архитектур байсан — ConvNets, LSTMs, MLPs, Transformers — харин одоо бид ихэвчлэн зөвхөн Transformer л ашигладаг<d-cite key="transformers"></d-cite>. Бид Transformer архитектурын бүх хэсгийг ойлгох нь маш чухал гэж итгэдэг: бүх матрицын яг хэмжээ, нормализаци хаана явагддаг, хэдэн параметр болон FLOPs<d-footnote>FLoating point OPs, энэ нь үндсэндээ нэмэх болон үржих үйлдлийн нийт тоо юм. Олон эх сурвалж FLOPs-ыг "секундэд үйлдэл" гэж ойлгодог ч бид FLOPs/s гэж бичихдээ үүнийг илүү тодорхой харуулдаг.</d-footnote> тус бүрт хэд байгаа вэ гэдгийг. [4-р хэсэг](transformers) энэ “Transformer-ийн математикийн” талаар нарийн тайлбарлаж, сургалт болон inference-д зориулж параметр болон FLOPs-ыг хэрхэн тоолохыг үзүүлнэ. Энэ нь бидний загвар хэр их санах ой ашиглах, тооцоолол эсвэл харилцаанд хэр их хугацаа зарцуулах, мөн attention нь feed-forward блокуудтай харьцуулахад хэзээ чухал болохыг хэлж өгнө.

{% include figure.liquid path="assets/img/transformer-diagram.png" class="img-fluid" caption="<b>Зураг:</b> стандарт Transformer давхарга, матриц үржүүлэг (matmul) бүрийг дугуй дотор цэгээр харуулсан. Бүх параметрүүд (нормоос бусад) нь ягаан өнгөөр байна. <a href='transformers'>4-р хэсэг</a> энэ зургийг дэлгэрэнгүй тайлбарлана." %}

[5-р хэсэг: Сургалт](training) болон [7-р хэсэг: Дүгнэлт](inference) нь энэ эссэгийн гол хэсэг бөгөөд бид үндсэн асуултыг хэлэлцэнэ: тодорхой хэмжээтэй загвар болон тодорхой тооны чип байгаа үед, би загвараа хэрхэн “хүчтэй өргөтгөх” горимд зэрэгцүүлэн ажиллуулах вэ? Энэ нь энгийн асуулт боловч хариулт нь гайхмаар төвөгтэй байдаг. Өндөр түвшинд авч үзвэл, загварыг олон чип дээр хуваахад ашигладаг 4 үндсэн зэрэгцүүлэх арга байдаг (**өгөгдлийн**, **тензорын**, **дамжуулах шугамын**, **экспертийн**), мөн санах ойн хэрэгцээг багасгах хэд хэдэн арга бий (**дахин материалжуулах**, **оновчлогч/загвар хуваах (өөрөөр хэлбэл ZeRO)**, **хост руу шилжүүлэх**, **градиент хуримтлуулах**). Бид эдгээрийн олон талаар энд хэлэлцэнэ.

Эдгээр хэсгүүдийн төгсгөлд та шинэ архитектур эсвэл тохиргоонд аль нэгийг нь өөрөө сонгож чадна гэж найдаж байна. [6-р хэсэг](applied-training) болон [8-р хэсэг](applied-inference) нь эдгээр ойлголтыг LLaMA-3 дээр хэрэгжүүлсэн практик заавар юм. LLaMA-3 нь алдартай, нээлттэй эхийн model юм.

Эцэст нь, [9-р хэсэг](profiling) болон [10-р хэсэг](jax-stuff) нь эдгээр санаануудыг JAX дээр хэрхэн хэрэгжүүлэх, мөн код буруу ажиллах үед хэрхэн profiling хийх, алдааг олох талаар үзнэ. [12-р хэсэг](gpus) нь шинэ хэсэг бөгөөд GPU-уудын талаар дэлгэрэнгүй тайлбарлана.

Бид танд өөрөө ажиллах бодлогуудыг өгөхийг хичээдэг. Та бүх хэсгийг заавал унших эсвэл дарааллаар нь унших шаардлагагүй. Мөн санал хүсэлтээ үлдээгээрэй. Одоогоор энэ бол ноорог бөгөөд цаашид засварлагдана. Баярлалаа!

*Бид энэ баримт бичигт байгаа олон санааг гаргасан Жэймс Брэдбюри болон Блэйк Хектман нарт талархал илэрхийлье.*

<h3 markdown=1 class="next-section">Цааш нь удаан хүлээлгүй, [энэ бол 1-р хэсэг](roofline) TPU roofline-уудын тухай.</h3>

## Хэсгүүд рүү холбоосууд

*Энэ цуврал магадгүй хэрэгтэйгээсээ урт байж болох ч бид таныг айлгахгүй гэж найдаж байна. Эхний гурван бүлэг нь урьдчилсан мэдлэг бөгөөд хэрвээ та мэддэг бол алгасаж болно, гэхдээ дараа нь ашиглах тэмдэглэгээг танилцуулдаг. Сүүлийн гурван хэсэг нь хамгийн хэрэгтэй байж магадгүй, учир нь эдгээр нь бодит загваруудтай хэрхэн ажиллахыг тайлбарладаг.*

**Хэсэг 1: Урьдчилсан мэдлэг**

* [**1-р бүлэг: Roofline шинжилгээний товч танилцуулга**](roofline). Алгоритмууд гурван зүйлээр хязгаарлагддаг: тооцоолол (compute), харилцаа (communication), санах ой (memory). Бид эдгээрийг ашиглан алгоритмууд маань хэр хурдан ажиллахыг ойролцоогоор тооцоолж болно.

* [**2-р бүлэг: TPU-ийг хэрхэн ойлгох вэ**](tpus). TPU хэрхэн ажилладаг вэ? Энэ нь бид ямар загваруудыг сургаж, ашиглаж болох вэ гэдэгт хэрхэн нөлөөлдөг вэ?

* [**3-р бүлэг: Хуваагдсан матриц ба тэдгээрийг хэрхэн үржүүлэх вэ**](sharding). Энд бид загвар хуваах болон олон TPU зэрэгцээ ажиллуулахыг дуртай үйлдлээрээ буюу (хуваагдсан) матрицын үржвэрээр тайлбарлана.

**Хэсэг 2: Трансформерүүд**

* [**4-р бүлэг: Трансформерийн математикийн мэдэх ёстой бүх зүйл**](transformers). Трансформер урд болон хойшоо дамжих үедээ хэдэн FLOPs ашигладаг вэ? Та параметрийн тоог тооцож чадах уу? KV кэшийн хэмжээг тооцож чадах уу? Бид энэ математикийг энд хамтдаа бодно.

* [**5-р бүлэг: Трансформерыг сургалтанд хэрхэн зэрэгцүүлэх вэ**](training). FSDP. Megatron sharding. Pipeline parallelism. Тодорхой тооны чип байгаа үед, тодорхой хэмжээтэй загвар, тодорхой batch size-тайгаар яаж хамгийн үр дүнтэйгээр сургах вэ?

* [**6-р бүлэг: LLaMA 3-ыг TPU дээр сургалт хийх**](applied-training). Бид LLaMA 3-ыг TPU дээр хэрхэн сургах вэ? Хэр удаан хугацаа шаардагдах вэ? Хэр их зардал гарах вэ?

* [**7-р бүлэг: Трансформерийн Инференсийн тухай бүх зүйл**](inference). Бид загвараа сургасны дараа, үүнийг ашиглах хэрэгтэй болдог. Инференс нь шинэ зүйл нэмж авчирдаг — саатал (latency) — мөн санах ойн хэрэглээг өөрчилдөг. Бид салангид сервер (disaggregated serving) хэрхэн ажилладаг, мөн KV кэш (KV cache)-ийн талаар хэрхэн бодох талаар ярилцана.

* [**8-р бүлэг: LLaMA 3-г TPU дээр ажиллуулах**](applied-inference). LLaMA 3-г TPU v5e дээр ажиллуулахад хэр их зардал гарах вэ? Хүлээлгийн хугацаа/дамжуулалтын хурдны солилцоо ямар байх вэ?

**Хэсэг 3: Практик хичээлүүд**

* [**9-р бүлэг: TPU кодыг хэрхэн профайл хийх вэ**](profiling). Жинхэнэ LLM-ууд дээрх онол шиг хэзээ ч энгийн биш байдаг. Энд бид JAX + XLA стекийг тайлбарлаж, JAX/TensorBoard профайлераар бодит асуудлыг хэрхэн шалгаж, засахыг үзүүлнэ.

* [**10-р бүлэг: JAX дээр TPU програмчлах**](jax-stuff). JAX нь тооцооллыг зэрэгцүүлэх олон ид шидийн API-уудыг өгдөг, гэхдээ та тэдгээрийг хэрхэн ашиглахыг мэдэх хэрэгтэй. Сонирхолтой жишээнүүд болон бодлогуудыг шийдсэн.

**4-р хэсэг: Дүгнэлт ба нэмэлт агуулга**

* [**11-р бүлэг: Дүгнэлт ба цаашдын унших материал**](conclusion). TPU болон LLM-ийн талаархи төгсгөлийн бодол, цааш унших материал.

* [**12-р бүлэг: GPU-гийн талаар хэрхэн бодох вэ**](gpus). GPU-гийн тухай нэмэлт хэсэг, тэд хэрхэн ажилладаг, хэрхэн сүлжээнд холбогддог, мөн тэдний roofline нь TPU-гаас хэрхэн ялгаатай болох талаар.

